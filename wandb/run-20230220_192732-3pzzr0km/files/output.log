Traceback (most recent call last):
  File "main.py", line 44, in <module>
    history = fit(epochs, net, train_ds, val_ds, device, opt = optimizer)
  File "/home/jeffreymo572/Kaggles/common/utils.py", line 125, in fit
    optimizer.step()
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/torch/optim/adam.py", line 408, in _single_tensor_adam
    denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt